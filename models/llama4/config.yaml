
# config.yaml
config_yaml:
  # Lightning Trainer configuration
  trainer:
    max_epochs: 100
    accelerator: gpu
    devices: 4
    strategy: ddp
    precision: 16-mixed
    gradient_clip_val: 1.0
    accumulate_grad_batches: 4
    val_check_interval: 1000
    limit_val_batches: 100
    enable_checkpointing: true
    logger: true

  # Model configuration
  model:
    model_args:
      dim: 4096
      n_layers: 32
      n_heads: 32
      n_kv_heads: 4
      vocab_size: 32000
      max_seq_len: 2048
      rope_theta: 500000
      use_scaled_rope: false
      norm_eps: 1e-5
    learning_rate: 1e-4
    weight_decay: 0.01
    warmup_steps: 1000

  # Data configuration
  data:
    train_data_path: "/path/to/train/data"
    val_data_path: "/path/to/val/data"
    batch_size: 32
    max_seq_len: 2048
    num_workers: 4

  # Optimizer and scheduler (optional, can override in model)
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      weight_decay: 0.01

  lr_scheduler:
    class_path: torch.optim.lr_scheduler.CosineAnnealingLR
    init_args:
      T_max: 100000


# model_configs/large.yaml
large_config:
  model:
    model_args:
      dim: 8192
      n_layers: 48
      n_heads: 64
      vocab_size: 50000
      max_seq_len: 4096
      use_scaled_rope: true
      rope_scaling_factor: 2.0
    learning_rate: 5e-5
    weight_decay: 0.05
    warmup_steps: 2000

